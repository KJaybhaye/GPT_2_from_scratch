{"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPTbTOOUS4GjGRJVhjAy8IG"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"99ce6d03445b4298b6ff70b12e001903":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f36e169271384d19ba15383b18f53bbb","IPY_MODEL_6c636260c4f24665b52c8a1f4435d21d","IPY_MODEL_cec907825bad49989e682bdff992f7e6"],"layout":"IPY_MODEL_cb0754415c7e4cf58568e8b9bbb9d2f4"}},"f36e169271384d19ba15383b18f53bbb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8b263ea87ddc4da8ae27fd8bc555cd5d","placeholder":"​","style":"IPY_MODEL_ea7080a12ed743b3829e3c8e912e561e","value":"config.json: 100%"}},"6c636260c4f24665b52c8a1f4435d21d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_86fd2e35e15a4b1b9bdf3fb92e1e27b3","max":665,"min":0,"orientation":"horizontal","style":"IPY_MODEL_67261dbf40294a7590feb19cf4aef6a5","value":665}},"cec907825bad49989e682bdff992f7e6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a4a2c679124a4201a63fcb218fd56d44","placeholder":"​","style":"IPY_MODEL_396badb68dc74ff2961e50c4f327a623","value":" 665/665 [00:00&lt;00:00, 10.1kB/s]"}},"cb0754415c7e4cf58568e8b9bbb9d2f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b263ea87ddc4da8ae27fd8bc555cd5d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea7080a12ed743b3829e3c8e912e561e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"86fd2e35e15a4b1b9bdf3fb92e1e27b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"67261dbf40294a7590feb19cf4aef6a5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a4a2c679124a4201a63fcb218fd56d44":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"396badb68dc74ff2961e50c4f327a623":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b1be102595b842d88b7b9f77a4846527":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_558f29d0afc44f3abc0f8f1b60052e76","IPY_MODEL_10484027ba914299811fe064110537c8","IPY_MODEL_e479b1773f8b42e0bb888b169fa1c4a0"],"layout":"IPY_MODEL_78377e2ad3b046168f27b4db015e731e"}},"558f29d0afc44f3abc0f8f1b60052e76":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bf586d3b8f9c412eb80cb9be44d9c4b2","placeholder":"​","style":"IPY_MODEL_3a1369c664c245c7b86c237a7097b497","value":"model.safetensors: 100%"}},"10484027ba914299811fe064110537c8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_44919e909f274c5a91a429f5a0aba00e","max":548105171,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b806038b85fe4d5abe38f463f11c914c","value":548105171}},"e479b1773f8b42e0bb888b169fa1c4a0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a10dd877d78d4e6c95a89aae2bc43f8e","placeholder":"​","style":"IPY_MODEL_14ad5a074d9c4524b3dae9739e1f3bf2","value":" 548M/548M [00:09&lt;00:00, 62.3MB/s]"}},"78377e2ad3b046168f27b4db015e731e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf586d3b8f9c412eb80cb9be44d9c4b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a1369c664c245c7b86c237a7097b497":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"44919e909f274c5a91a429f5a0aba00e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b806038b85fe4d5abe38f463f11c914c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a10dd877d78d4e6c95a89aae2bc43f8e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"14ad5a074d9c4524b3dae9739e1f3bf2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"45fd3f60bbcc4883a93c7577103b8446":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e44bb63e902c4a19a9fd8513e999f142","IPY_MODEL_9ff9c40b351940d4896560620d113550","IPY_MODEL_51e4043c2cf34f5c8d176e5497e4e543"],"layout":"IPY_MODEL_d320959d044546e3aea6cda2ab07c38c"}},"e44bb63e902c4a19a9fd8513e999f142":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9377d6396eb8460080a9465eba733344","placeholder":"​","style":"IPY_MODEL_499143bf925e41219e075873d1c2f6d1","value":"generation_config.json: 100%"}},"9ff9c40b351940d4896560620d113550":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_eb10338e038249e08827c919fe0e9fdb","max":124,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a1e9697e0ba8486b8dce5f10ad57fe73","value":124}},"51e4043c2cf34f5c8d176e5497e4e543":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_96874f7edbc5484c84f798d9013db70b","placeholder":"​","style":"IPY_MODEL_a98d14dabdb8473c86fc3f6cefe4ae6e","value":" 124/124 [00:00&lt;00:00, 3.15kB/s]"}},"d320959d044546e3aea6cda2ab07c38c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9377d6396eb8460080a9465eba733344":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"499143bf925e41219e075873d1c2f6d1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eb10338e038249e08827c919fe0e9fdb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1e9697e0ba8486b8dce5f10ad57fe73":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"96874f7edbc5484c84f798d9013db70b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a98d14dabdb8473c86fc3f6cefe4ae6e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"20d09b77d4b844b3beb573f06741a823":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9b2f3ae0604d4bfea6ac9610c262b3cc","IPY_MODEL_65790e0649fb40369903e89043575c4a","IPY_MODEL_b14b9aefcf574273bc9a31bdfded977d"],"layout":"IPY_MODEL_1d54e83fd2db4e5dba76b8de6461ef99"}},"9b2f3ae0604d4bfea6ac9610c262b3cc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aadbfefa76554c2b96d7daf6d95e834e","placeholder":"​","style":"IPY_MODEL_a77d916334b7484fb71057fd63c1d948","value":"tokenizer_config.json: 100%"}},"65790e0649fb40369903e89043575c4a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c2896dec15d47a787b8ccdbd3a3dd4d","max":26,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7a53b7de62ac4efc9f3c1076cbe03c5e","value":26}},"b14b9aefcf574273bc9a31bdfded977d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_40b6d17f1ebb423a9a98e486d36bb264","placeholder":"​","style":"IPY_MODEL_08906f9432d54c54b7ebc9112be5d29d","value":" 26.0/26.0 [00:00&lt;00:00, 384B/s]"}},"1d54e83fd2db4e5dba76b8de6461ef99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aadbfefa76554c2b96d7daf6d95e834e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a77d916334b7484fb71057fd63c1d948":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8c2896dec15d47a787b8ccdbd3a3dd4d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7a53b7de62ac4efc9f3c1076cbe03c5e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"40b6d17f1ebb423a9a98e486d36bb264":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08906f9432d54c54b7ebc9112be5d29d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1127c45cc98b40f1877e26cc04743e4b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f7fda730df6742a79ac813993e520bcb","IPY_MODEL_0a46176fb0194811baa1f31a38cb3a8d","IPY_MODEL_00049b37f4f54f8897df50d3f5dea47e"],"layout":"IPY_MODEL_469222831f08499ca88468245091709e"}},"f7fda730df6742a79ac813993e520bcb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c88cc70b0cd844d7a8d6e2419d389286","placeholder":"​","style":"IPY_MODEL_3592019b68bd4dc18167205899c78ed0","value":"vocab.json: 100%"}},"0a46176fb0194811baa1f31a38cb3a8d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b4a7dc08cff042d59a32acf1380a3136","max":1042301,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7691dead70f648d9a59aeb398402d7b7","value":1042301}},"00049b37f4f54f8897df50d3f5dea47e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c653390ecbf4c529e18864565b13d52","placeholder":"​","style":"IPY_MODEL_b4c10ccd3e3448c99658aec7b5e86968","value":" 1.04M/1.04M [00:00&lt;00:00, 1.52MB/s]"}},"469222831f08499ca88468245091709e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c88cc70b0cd844d7a8d6e2419d389286":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3592019b68bd4dc18167205899c78ed0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b4a7dc08cff042d59a32acf1380a3136":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7691dead70f648d9a59aeb398402d7b7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5c653390ecbf4c529e18864565b13d52":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4c10ccd3e3448c99658aec7b5e86968":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c2396ef2f6564d128138c8b8c9591d3a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_88c659d8050d4b9b8215457a20203972","IPY_MODEL_dd3023695ca9414280ecac9ded3a65d3","IPY_MODEL_455b8ac0f8a74359896d0fa5989e9523"],"layout":"IPY_MODEL_25a3d193e4634b9f918f762746b91479"}},"88c659d8050d4b9b8215457a20203972":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0a280dcff23a43bdb08c41703d14728e","placeholder":"​","style":"IPY_MODEL_070af7144c984d42a3d3d92edb36d00b","value":"merges.txt: 100%"}},"dd3023695ca9414280ecac9ded3a65d3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_638388f428b74206acfada3a6a42e89b","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3a3ccf21410a420aae2706bd0ac1b572","value":456318}},"455b8ac0f8a74359896d0fa5989e9523":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_06e7876bef29474d8535d93411c65810","placeholder":"​","style":"IPY_MODEL_086c70b15fe9420d9634d020b2e217a2","value":" 456k/456k [00:00&lt;00:00, 905kB/s]"}},"25a3d193e4634b9f918f762746b91479":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a280dcff23a43bdb08c41703d14728e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"070af7144c984d42a3d3d92edb36d00b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"638388f428b74206acfada3a6a42e89b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a3ccf21410a420aae2706bd0ac1b572":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"06e7876bef29474d8535d93411c65810":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"086c70b15fe9420d9634d020b2e217a2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c4cd228a699c478993a35f9548149fd2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_45c8b2f1aa5e4e99badb3f89b89789b4","IPY_MODEL_34963ad8a50f48e2a6bd3778c6b1fa34","IPY_MODEL_f77a37a2cef2432093b18d28fd24ae77"],"layout":"IPY_MODEL_d596e5d77e5144988e4030388bdfbe57"}},"45c8b2f1aa5e4e99badb3f89b89789b4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_89c422e0d9824ffbbaaac5c41bf524cb","placeholder":"​","style":"IPY_MODEL_5c4b517f44b34532bfad74ce6a2505ab","value":"tokenizer.json: 100%"}},"34963ad8a50f48e2a6bd3778c6b1fa34":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_df53809ca15d4615b387d561fdc24fae","max":1355256,"min":0,"orientation":"horizontal","style":"IPY_MODEL_37fe73b25f204bfb85dd96fd1e91d97c","value":1355256}},"f77a37a2cef2432093b18d28fd24ae77":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_54b1193674ff4e2c917df5324b94c716","placeholder":"​","style":"IPY_MODEL_a80f82350d7441cab6f239a572f603ac","value":" 1.36M/1.36M [00:00&lt;00:00, 1.98MB/s]"}},"d596e5d77e5144988e4030388bdfbe57":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89c422e0d9824ffbbaaac5c41bf524cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c4b517f44b34532bfad74ce6a2505ab":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"df53809ca15d4615b387d561fdc24fae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37fe73b25f204bfb85dd96fd1e91d97c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"54b1193674ff4e2c917df5324b94c716":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a80f82350d7441cab6f239a572f603ac":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8998077,"sourceType":"datasetVersion","datasetId":5420135},{"sourceId":9339647,"sourceType":"datasetVersion","datasetId":5659947},{"sourceId":196899762,"sourceType":"kernelVersion"}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip3 install transformers datasets","metadata":{"_uuid":"f5355f82-5888-4e15-9ece-f67a359c8133","_cell_guid":"ebffc7be-a05c-4a5e-a127-ac55516ea876","collapsed":false,"id":"uYg3UmY6rq4H","executionInfo":{"status":"ok","timestamp":1725714489154,"user_tz":-330,"elapsed":13271,"user":{"displayName":"K Jaybhaye","userId":"07761635259615291325"}},"outputId":"f6be7b41-e50c-4b0b-b8a1-879f3c74971b","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-18T12:34:53.031323Z","iopub.execute_input":"2024-09-18T12:34:53.032227Z","iopub.status.idle":"2024-09-18T12:35:06.440069Z","shell.execute_reply.started":"2024-09-18T12:34:53.032174Z","shell.execute_reply":"2024-09-18T12:35:06.438945Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.44.0)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.21.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.24.6)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.4)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom dataclasses import dataclass\nimport math\nimport time\nfrom tqdm import tqdm\nimport numpy as np\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline, set_seed\n\nfrom torch.distributed import init_process_group, destroy_process_group\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nimport os\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\ntorch.manual_seed(42)\nif torch.cuda.is_available():\n  torch.cuda.manual_seed_all(42)","metadata":{"_uuid":"a0056d68-ab81-440f-b2ae-d85479577ee0","_cell_guid":"5cd514cb-0f10-4a0b-a91d-0d77f6289cfc","collapsed":false,"id":"z1TSVHHJt2hl","executionInfo":{"status":"ok","timestamp":1725714512322,"user_tz":-330,"elapsed":23173,"user":{"displayName":"K Jaybhaye","userId":"07761635259615291325"}},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-18T12:35:06.442047Z","iopub.execute_input":"2024-09-18T12:35:06.442439Z","iopub.status.idle":"2024-09-18T12:35:06.457810Z","shell.execute_reply.started":"2024-09-18T12:35:06.442403Z","shell.execute_reply":"2024-09-18T12:35:06.456896Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Check gpt 2 model from hugging face","metadata":{}},{"cell_type":"code","source":"model = GPT2LMHeadModel.from_pretrained('gpt2')\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n\nweights = model.state_dict()\nfor ind, (k,v) in enumerate(weights.items()):\n  print(k, v.shape)\n  if ind > 10:\n    break","metadata":{"_uuid":"84177a7c-0692-41a1-bb97-2c3726e74cc3","_cell_guid":"cd0fde34-c6db-407d-8c3f-69ed87c42d18","collapsed":false,"id":"-wPXbKVTrnyf","executionInfo":{"status":"ok","timestamp":1725714531665,"user_tz":-330,"elapsed":19347,"user":{"displayName":"K Jaybhaye","userId":"07761635259615291325"}},"outputId":"ff77f95d-0816-4a3c-bb06-ccdb836183fc","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-18T12:35:06.821394Z","iopub.execute_input":"2024-09-18T12:35:06.822278Z","iopub.status.idle":"2024-09-18T12:35:06.827986Z","shell.execute_reply.started":"2024-09-18T12:35:06.822239Z","shell.execute_reply":"2024-09-18T12:35:06.826946Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"\ntransformer.wte.weight torch.Size([50257, 768])\ntransformer.wpe.weight torch.Size([1024, 768])\ntransformer.h.0.ln_1.weight torch.Size([768])\ntransformer.h.0.ln_1.bias torch.Size([768])\ntransformer.h.0.attn.c_attn.weight torch.Size([768, 2304])\ntransformer.h.0.attn.c_attn.bias torch.Size([2304])\ntransformer.h.0.attn.c_proj.weight torch.Size([768, 768])\ntransformer.h.0.attn.c_proj.bias torch.Size([768])\ntransformer.h.0.ln_2.weight torch.Size([768])\ntransformer.h.0.ln_2.bias torch.Size([768])\ntransformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])\ntransformer.h.0.mlp.c_fc.bias torch.Size([3072])\n\n","output_type":"stream"}]},{"cell_type":"code","source":"pip = pipeline('text-generation', model=model, tokenizer=tokenizer, device=device, truncation=True)\nset_seed(42)\nout = pip(\"researcher found that nuclear energy\", max_length=150, num_return_sequences=1)\nprint(out[0]['generated_text'])","metadata":{"_uuid":"29413548-98fe-472f-8982-4d7649945706","_cell_guid":"cc23ada2-b915-4646-8803-dc32cd938620","collapsed":false,"id":"0XUlUQWJt0oq","executionInfo":{"status":"ok","timestamp":1725714560969,"user_tz":-330,"elapsed":29309,"user":{"displayName":"K Jaybhaye","userId":"07761635259615291325"}},"outputId":"c89d0cda-707b-4412-849d-951ff389f75a","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-18T12:35:07.126325Z","iopub.execute_input":"2024-09-18T12:35:07.127366Z","iopub.status.idle":"2024-09-18T12:35:07.132943Z","shell.execute_reply.started":"2024-09-18T12:35:07.127323Z","shell.execute_reply":"2024-09-18T12:35:07.131845Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"researcher found that nuclear energy, the only source of energy in the solar system, would cost about $2 trillion if all of its residents would join the public sector, the most for a decade, said Robert Weiersz, a professor of economics at the University of California, Berkeley and one of the world's leading nuclear experts, in an April interview with The National Security Archive. But at that price, he said, many residents in Texas would not be willing to pay that much.\n\n\"If your health insurance covers nuclear energy, for example, you won't pay more if you're covered by the health insurance package,\" Weiersz said. \"If you're able to get cancer care for free, you'll still be\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## GPT From Scratch","metadata":{"_uuid":"9c9ea314-241f-4fa2-b34e-beff568a3204","_cell_guid":"966ccb44-c2a9-43d7-b8c8-21e486f32601","trusted":true}},{"cell_type":"code","source":"@dataclass\nclass GPTConfig:\n  vocab_size: int = 50257\n  block_size: int = 1024 #max number of tokens in input and output\n  n_layer: int = 12\n  n_head: int = 12\n  n_embd: int = 768","metadata":{"_uuid":"f59650d6-d8a0-42a9-ab04-f2a9d58b34b1","_cell_guid":"9529f6dc-4abe-456a-89a9-f5efb790e9e2","collapsed":false,"id":"UaJXNtN5wtIZ","executionInfo":{"status":"ok","timestamp":1725714560970,"user_tz":-330,"elapsed":14,"user":{"displayName":"K Jaybhaye","userId":"07761635259615291325"}},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-18T12:35:07.755991Z","iopub.execute_input":"2024-09-18T12:35:07.756369Z","iopub.status.idle":"2024-09-18T12:35:07.762606Z","shell.execute_reply.started":"2024-09-18T12:35:07.756332Z","shell.execute_reply":"2024-09-18T12:35:07.761639Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class FF(nn.Module):\n    def __init__(self, config: GPTConfig):\n        super(FF, self).__init__()\n        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)  # input and output neurons, inp to 4 inp\n        self.gelu = nn.GELU(approximate='tanh')\n        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n        self.c_proj.GPTSCALED_INIT = True # to compensate the increase of standard deviation in residual stream\n\n    def forward(self, x):\n        x = self.c_fc(x)\n        x = self.gelu(x)\n        x = self.c_proj(x)\n        return x","metadata":{"_uuid":"72513854-d3ef-4833-989e-0abf475e1c21","_cell_guid":"f42f1698-afd8-476e-bd2e-634fdbf713fa","collapsed":false,"id":"FwH5JJuu4EJ1","executionInfo":{"status":"ok","timestamp":1725714560970,"user_tz":-330,"elapsed":14,"user":{"displayName":"K Jaybhaye","userId":"07761635259615291325"}},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-18T12:35:08.111304Z","iopub.execute_input":"2024-09-18T12:35:08.111701Z","iopub.status.idle":"2024-09-18T12:35:08.118902Z","shell.execute_reply.started":"2024-09-18T12:35:08.111664Z","shell.execute_reply":"2024-09-18T12:35:08.117854Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class SelfAttention(nn.Module):\n  def __init__(self, config: GPTConfig):\n    super().__init__()\n    assert config.n_embd % config.n_head == 0 # n_embd = num_heads * head_size\n    self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd) #key, value, query projection (3*k size)\n    self.c_proj = nn.Linear(config.n_embd, config.n_embd) #output projection\n    self.c_proj.GPTSCALED_INIT = True\n\n    self.n_head = config.n_head\n    self.n_embd = config.n_embd\n\n    #for parameters that shouldent be updated by optimizer but should be in state_dict (here mask)\n    self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n                         .view(1, 1, config.block_size, config.block_size))  # mask with size max_tokens x max_tokens\n\n    #torch.tril Returns lower triangular part of the matrix or batch of matrices input, other elements are set to 0\n\n\n  def forward(self, x):\n    B, T, C = x.size() #shape is alise for size (batch, sequence length, n_embd)\n    # print(f\"b: {B}, t: {T}, c: {C}\")\n    q, k, v = self.c_attn(x).split(self.n_embd, dim=2) # as out of c_att is all three concatnated\n\n    # n_embd = num_heads * head_size\n    q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n    k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n    v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n\n    # attn = (q @ k.transpose(-2, -1)) * (1.0/ math.sqrt(k.size(-1))) # (B, nh, T, T)\n    # attn = attn.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf')) #masking\n    # attn = attn.softmax(dim=-1)\n    # y = attn @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n    y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n\n    y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n\n    return self.c_proj(y)","metadata":{"_uuid":"4647f149-c0a5-4867-94f4-a24e5ec329ae","_cell_guid":"9c0289d7-f182-4baf-a166-f8521aca6c45","collapsed":false,"id":"UKLzc8Kp1tbD","executionInfo":{"status":"ok","timestamp":1725714560970,"user_tz":-330,"elapsed":14,"user":{"displayName":"K Jaybhaye","userId":"07761635259615291325"}},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-18T12:35:08.652774Z","iopub.execute_input":"2024-09-18T12:35:08.653151Z","iopub.status.idle":"2024-09-18T12:35:08.663869Z","shell.execute_reply.started":"2024-09-18T12:35:08.653115Z","shell.execute_reply":"2024-09-18T12:35:08.663033Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class Block(nn.Module): #decoder block\n  def __init__(self, config: GPTConfig):\n    super().__init__()\n    self.ln_1 = nn.LayerNorm(config.n_embd)\n    self.attn = SelfAttention(config)\n    self.ln_2 = nn.LayerNorm(config.n_embd)\n\n    self.mlp = FF(config)\n\n  def forward(self, x):\n    x = x + self.attn(self.ln_1(x))\n    x = x + self.mlp(self.ln_2(x))\n    return x","metadata":{"_uuid":"1d42fc56-f80b-42dc-b528-cf1bce5f8100","_cell_guid":"95190603-717b-45b4-97b6-e0f8e21af2fc","collapsed":false,"id":"RC35aeyHzW3m","executionInfo":{"status":"ok","timestamp":1725714560970,"user_tz":-330,"elapsed":13,"user":{"displayName":"K Jaybhaye","userId":"07761635259615291325"}},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-18T12:35:08.873576Z","iopub.execute_input":"2024-09-18T12:35:08.874343Z","iopub.status.idle":"2024-09-18T12:35:08.880287Z","shell.execute_reply.started":"2024-09-18T12:35:08.874305Z","shell.execute_reply":"2024-09-18T12:35:08.879317Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class GPT(nn.Module):\n  def __init__(self, config: GPTConfig):\n    super().__init__()\n    self.config = config\n\n    self.transformer = nn.ModuleDict({\n        'wte': nn.Embedding(config.vocab_size, config.n_embd), # token embeddings vocab size x embedding dimension\n        'wpe': nn.Embedding(config.block_size, config.n_embd),  # positional embeddings\n        'h': nn.ModuleList([Block(config) for _ in range(config.n_layer)]), # n_layer (6) decoder blocks\n        'ln_f': nn.LayerNorm(config.n_embd),\n        })\n    self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False) #final head embedding dims x vocab size\n\n    # weight sharing scheme\n    self.transformer['wte'].weight = self.lm_head.weight\n\n    self.apply(self._init_weights)\n\n  def _init_weights(self, module):\n    if isinstance(module, nn.Linear):\n      std = 0.02\n      if hasattr(module, 'GPTSCALED_INIT'):\n        if module.GPTSCALED_INIT:\n          std *= (2 * self.config.n_layer) ** -0.5\n      nn.init.normal_(module.weight, mean=0.0, std=std)\n      if module.bias is not None:\n        nn.init.zeros_(module.bias)\n    elif isinstance(module, nn.Embedding):\n      nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n  def forward(self, x, targets=None):\n    B, T = x.size()\n    assert T <= self.config.block_size, \"Embedding size is wrong\"\n    tok_emb = self.transformer['wte'](x)\n    # print(tok_emb.shape)\n    pos_emb = self.transformer['wpe'](torch.arange(0, T, device=x.device))\n    # print(pos_emb.shape)\n    x = tok_emb + pos_emb\n    # print('tok + pos', x.shape)\n    for block in self.transformer['h']:\n      x = block(x)\n\n    # print('blocks out', x.shape)\n    x = self.transformer['ln_f'](x)\n    # print('ln_f out', x.shape)\n    logits = self.lm_head(x)\n    # print('lm_head out', logits.shape)\n    loss = None\n    if targets is not None:\n      loss = nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n    return logits, loss\n\n  @classmethod\n  def from_pretrained(cls, model_type):\n    assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n    from transformers import GPT2LMHeadModel\n    print(\"loading weights from pretrained gpt: %s\" % model_type)\n\n    config_args = {\n          'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n          'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n          'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n          'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n      }[model_type]\n    config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n    config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n    config = GPTConfig(**config_args)\n    model = GPT(config)\n    sd = model.state_dict()\n    sd_keys = sd.keys()\n    sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n    # init a huggingface/transformers model\n    model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n    sd_hf = model_hf.state_dict()\n\n    # copy while ensuring all of the parameters are aligned and match in names and shapes\n    sd_keys_hf = sd_hf.keys()\n    sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n    sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n    transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n    # openai checkpoints use a \"Conv1D\" module, but we use a vanilla Linear\n    # this means that we have to transpose these weights when we import them\n    assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n    for k in sd_keys_hf:\n      if any(k.endswith(w) for w in transposed):\n      # special treatment for the Conv1D weights we need to transpose\n        assert sd_hf[k].shape[::-1] == sd[k].shape\n        with torch.no_grad():\n          sd[k].copy_(sd_hf[k].t())\n      else:\n        # vanilla copy over the other parameters\n        assert sd_hf[k].shape == sd[k].shape\n        with torch.no_grad():\n          sd[k].copy_(sd_hf[k])\n\n    return model","metadata":{"_uuid":"47754622-81d7-4709-836c-d23f6e683da1","_cell_guid":"c7adebe6-2bbd-4b13-b4e9-73a2dcfd06a2","collapsed":false,"id":"FyMoO9gOxYMH","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-18T12:35:09.348359Z","iopub.execute_input":"2024-09-18T12:35:09.348761Z","iopub.status.idle":"2024-09-18T12:35:09.370357Z","shell.execute_reply.started":"2024-09-18T12:35:09.348722Z","shell.execute_reply":"2024-09-18T12:35:09.369274Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def sample(input_string, model, tokenizer, num_return_sequences=1, max_length=200):\n  tokens = tokenizer.encode(input_string)\n  tokens = torch.tensor(tokens, dtype=torch.long)\n  tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n\n  x = tokens.to(device)\n  while x.size(1) < max_length:\n    with torch.no_grad():\n      logits, _ = model(x)\n      logits = logits[:, -1, :]\n      probs = F.softmax(logits, dim=-1)\n\n      topk_probs, topk_ix = probs.topk(50)\n      ix = torch.multinomial(topk_probs, num_samples=1)\n      xcol = torch.gather(topk_ix, dim=-1, index=ix)\n      x = torch.cat((x, xcol), dim=1)\n\n  outputs = []\n  for i in range(num_return_sequences):\n    tokens = x[i, :max_length].tolist()\n    outputs.append(tokenizer.decode(tokens))\n\n  return outputs","metadata":{"_uuid":"9e147c96-34dd-4b0e-8824-427168ab8606","_cell_guid":"02195408-174c-4bbb-b7ba-f0c07e37a6c0","collapsed":false,"id":"b_jZV7ZS8Byo","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-18T12:35:09.576053Z","iopub.execute_input":"2024-09-18T12:35:09.576441Z","iopub.status.idle":"2024-09-18T12:35:09.584876Z","shell.execute_reply.started":"2024-09-18T12:35:09.576393Z","shell.execute_reply":"2024-09-18T12:35:09.583903Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"model = GPT(GPTConfig())\n\nweights = model.state_dict()\n\nfor ind, (k,v) in enumerate(weights.items()):\n  print(k, v.shape)\n  if ind > 10:\n    break","metadata":{"_uuid":"03afac7f-90f5-438c-b0d1-7deae8d15739","_cell_guid":"8b53d5bc-ac44-46c4-85ae-c2592b9a343c","collapsed":false,"id":"J4bu1fRxCo2B","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-18T12:35:12.701299Z","iopub.execute_input":"2024-09-18T12:35:12.701690Z","iopub.status.idle":"2024-09-18T12:35:12.707649Z","shell.execute_reply.started":"2024-09-18T12:35:12.701654Z","shell.execute_reply":"2024-09-18T12:35:12.706481Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"transformer.wte.weight torch.Size([50257, 768])\ntransformer.wpe.weight torch.Size([1024, 768])\ntransformer.h.0.ln_1.weight torch.Size([768])\ntransformer.h.0.ln_1.bias torch.Size([768])\ntransformer.h.0.attn.bias torch.Size([1, 1, 1024, 1024])\ntransformer.h.0.attn.c_attn.weight torch.Size([2304, 768])\ntransformer.h.0.attn.c_attn.bias torch.Size([2304])\ntransformer.h.0.attn.c_proj.weight torch.Size([768, 768])\ntransformer.h.0.attn.c_proj.bias torch.Size([768])\ntransformer.h.0.ln_2.weight torch.Size([768])\ntransformer.h.0.ln_2.bias torch.Size([768])\ntransformer.h.0.mlp.c_fc.weight torch.Size([3072, 768])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Layers are same as the huggingface model ","metadata":{}},{"cell_type":"code","source":"model = GPT.from_pretrained('gpt2')\nmodel = model.to(device)\n\ntokenizer =  GPT2Tokenizer.from_pretrained('gpt2')","metadata":{"_uuid":"234091d7-c738-4e2f-bb97-974305470657","_cell_guid":"9c2e4fbc-423f-40af-af03-8b042e8e71b8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-18T12:35:20.276290Z","iopub.execute_input":"2024-09-18T12:35:20.276651Z","iopub.status.idle":"2024-09-18T12:35:20.281608Z","shell.execute_reply.started":"2024-09-18T12:35:20.276619Z","shell.execute_reply":"2024-09-18T12:35:20.280667Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"loading weights from pretrained gpt: gpt2\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Since the model can be initialized using huggingface weights. So the the gpt model has correct structure.","metadata":{}},{"cell_type":"code","source":"samples = sample(\"researcher found that nuclear energy\", model, tokenizer, num_return_sequences=1, max_length=150)\n\nfor sam in samples:\n  print(sam)","metadata":{"_uuid":"5a6552e7-d14d-416c-b699-42d3b885b0be","_cell_guid":"06c76939-77d5-406e-807b-6a9d52a6c715","collapsed":false,"id":"N8-gkTud81C1","executionInfo":{"status":"ok","timestamp":1725284403134,"user_tz":-330,"elapsed":506,"user":{"displayName":"K Jaybhaye","userId":"07761635259615291325"}},"outputId":"c43e1535-6668-4bb4-b821-2f251083564f","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-18T12:35:22.716344Z","iopub.execute_input":"2024-09-18T12:35:22.716714Z","iopub.status.idle":"2024-09-18T12:35:22.722584Z","shell.execute_reply.started":"2024-09-18T12:35:22.716680Z","shell.execute_reply":"2024-09-18T12:35:22.721664Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"researcher found that nuclear energy's efficiency had reached its peak at the end of its life cycle.\n\nThe new power station would have three operating reactors or \"situational reactor cores\" and had five thousand tons of water in its tanks and tanks.\n\nScientists have known for years that nuclear's capacity exceeded that of coal and natural gas until recently, and then continued to grow.\n\n\"This will be a unique solution in many ways to meet the requirements of many energy producers in the future,\" said Lawrence Krauss, senior scientist for the nuclear industry.\n\nThe U.S. Energy Information Administration (EIA) estimates that more than half of all nuclear power-related energy consumption today comes from renewable sources,\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Samplt training on text data","metadata":{}},{"cell_type":"code","source":"with open('/kaggle/input/shakespear-text/shakspear.txt', 'r', encoding='utf-8') as f:\n  text = f.read()\n\ndata = text[:1000]\nprint(data[:100])","metadata":{"_uuid":"5ddc8d27-8cfc-4698-8653-4965916448b0","_cell_guid":"9c84256b-7643-480d-b8fb-48ea11875b1e","collapsed":false,"id":"iwIPDpft0-UY","executionInfo":{"status":"ok","timestamp":1725285117978,"user_tz":-330,"elapsed":3,"user":{"displayName":"K Jaybhaye","userId":"07761635259615291325"}},"outputId":"e4bc9595-5d18-4388-84e8-318fd2ad6abe","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-18T12:35:24.416375Z","iopub.execute_input":"2024-09-18T12:35:24.417303Z","iopub.status.idle":"2024-09-18T12:35:24.597278Z","shell.execute_reply.started":"2024-09-18T12:35:24.417262Z","shell.execute_reply":"2024-09-18T12:35:24.596318Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"THE SONNETS\n\n                    1\n\nFrom fairest creatures we desire increase,\nThat thereby beauty’s\n","output_type":"stream"}]},{"cell_type":"code","source":"B, T = 4, 32\n\ntokens = tokenizer.encode(data)\ntokens = torch.tensor(tokens[:B*T + 1], dtype=torch.long)\nx = tokens[:-1].view(B, T)\ny = tokens[1:].view(B, T)\n# x, y\n\nmodel = GPT(GPTConfig()).to(device)\nlogits, loss = model(x.to(device), y.to(device))\nprint(logits.shape, loss)","metadata":{"_uuid":"bee59080-ac13-46af-8976-556f06225f8f","_cell_guid":"e7db5e4c-e690-4eb9-ae26-6a1cb846c124","collapsed":false,"id":"-RNOO9aJCoBQ","executionInfo":{"status":"ok","timestamp":1725286244355,"user_tz":-330,"elapsed":6560,"user":{"displayName":"K Jaybhaye","userId":"07761635259615291325"}},"outputId":"85587c5a-496f-45cc-aec7-aee06d7d2612","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-18T12:35:26.806257Z","iopub.execute_input":"2024-09-18T12:35:26.806882Z","iopub.status.idle":"2024-09-18T12:35:26.811464Z","shell.execute_reply.started":"2024-09-18T12:35:26.806841Z","shell.execute_reply":"2024-09-18T12:35:26.810605Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"torch.Size([4, 32, 50257]) tensor(10.9023, device='cuda:0', grad_fn=<NllLossBackward0>)\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(mod_0.parameters(), lr=1e-3)\nfor i in range(6):\n  print(f\"epoch: {i}\")\n  optimizer.zero_grad()\n  logits, loss = model.to(device), y.to(device))\n  loss.backward()\n  optimizer.step()\n  print(loss.item())","metadata":{"_uuid":"1858df02-39b1-4d6a-862b-7eb9266ba126","_cell_guid":"28898e33-1a2c-445e-b4d4-c40d8cb3b7bc","collapsed":false,"id":"tHSyhFptF6_Y","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-18T12:35:27.396539Z","iopub.execute_input":"2024-09-18T12:35:27.397199Z","iopub.status.idle":"2024-09-18T12:35:27.402617Z","shell.execute_reply.started":"2024-09-18T12:35:27.397160Z","shell.execute_reply":"2024-09-18T12:35:27.401626Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"epoch: 0\n3.77512788772583\nepoch: 1\n5.781515121459961\nepoch: 2\n4.745081424713135\nepoch: 3\n3.923518180847168\nepoch: 4\n3.863071918487549\nepoch: 5\n3.870129346847534\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Training seems to be working properly.","metadata":{}},{"cell_type":"markdown","source":"## Sample distriubted training using DistributedDataParallel","metadata":{"_uuid":"979497d6-4d19-4da5-a3c4-7574037ce2a9","_cell_guid":"a30e06d2-3421-413d-8169-8a65654e249f","trusted":true}},{"cell_type":"code","source":"class DistributedDataloader:\n  def __init__(self, txt, tokenizer, batch_size, seq_length, process_rank = 0 , num_processes = 1, split=\"train\", split_portion=0.8):\n    with open(txt, 'r', encoding='utf-8') as f:\n      text = f.read()\n    \n    assert split in ['train', 'val'] \n    tokens = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n    \n    if split == 'train':\n        self.tokens = tokens[: int(len(tokens) * split_portion)]\n    else:\n        self.tokens = tokens[int(len(tokens) * split_portion):]\n\n    self.n_tokens = len(self.tokens)\n    self.batch_size = batch_size\n    self.seq_length = seq_length\n    self.num_batches = self.n_tokens // (batch_size * seq_length)\n\n    self.ind = batch_size * seq_length * process_rank\n    \n    self.process_rank = process_rank\n    self.num_processes = num_processes\n\n    print(\"Number of tokens: \", self.n_tokens)\n    print(\"Number of Batches: \", self.num_batches)\n\n  def __iter__(self):\n    return self\n\n  def __len__(self):\n    return self.num_batches\n\n  def __next__(self):\n    B, T = self.batch_size, self.seq_length\n    if self.ind >= self.n_tokens - B * T * self.num_processes - 1:\n      self.ind = self.batch_size * self.seq_length * self.process_rank\n#       raise StopIteration\n    BT = self.tokens[self.ind : self.ind + B * T + 1]\n    self.ind += B * T * self.num_processes\n\n    x = BT[:-1].view(B, T)\n    y = BT[1:].view(B, T)\n\n    return x, y","metadata":{"_uuid":"47008bd7-eaa2-4a9d-9228-8ee1971392cb","_cell_guid":"baba4c6e-615b-4bc7-8f05-185c4b248ef5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-18T12:35:33.141393Z","iopub.execute_input":"2024-09-18T12:35:33.141802Z","iopub.status.idle":"2024-09-18T12:35:33.153049Z","shell.execute_reply.started":"2024-09-18T12:35:33.141764Z","shell.execute_reply":"2024-09-18T12:35:33.151909Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"import inspect\n\ndef get_optimizer(model, lr, weight_decay=0.1, device='cpu', betas = (0.9, 0.95)):\n  params_dict = {pn: p for pn, p in model.named_parameters() if p.requires_grad}\n  # all weight tensors in matmuls + embeddings decay, all biases and layernorms don't\n  decay_params = [p for n, p in params_dict.items() if p.dim() >= 2]\n  nodecay_params = [p for n, p in params_dict.items() if p.dim() < 2]\n\n  optim_groups = [\n      {\"params\": decay_params, \"weight_decay\": weight_decay},\n      {\"params\": nodecay_params, \"weight_decay\": 0.0},\n  ]\n  fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n  use_fused = fused_available and device == 'cuda'\n\n  optimizer = torch.optim.AdamW(optim_groups, lr=lr, betas=betas, fused=use_fused)\n  return optimizer","metadata":{"_uuid":"ed9f8e6e-43cc-4387-9177-b5f4e3c9ff15","_cell_guid":"001c80d3-336f-4352-a814-a3920c48f9bb","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-18T12:35:34.246288Z","iopub.execute_input":"2024-09-18T12:35:34.247014Z","iopub.status.idle":"2024-09-18T12:35:34.254284Z","shell.execute_reply.started":"2024-09-18T12:35:34.246971Z","shell.execute_reply":"2024-09-18T12:35:34.253289Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"max_lr = 6e-4\nmin_lr = max_lr * 0.1\nwarmup_steps = 10 #715\nmax_steps = 100 # 19073\n\n\ndef get_lr(it): # cosine schedular\n  if it < warmup_steps:\n    return max_lr * (it+1) / warmup_steps\n  if it > max_steps:\n    return min_lr\n  decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n  assert 0 <= decay_ratio <= 1\n  coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n  return min_lr + coeff * (max_lr - min_lr)","metadata":{"_uuid":"fc48d9e3-2ffe-438b-b7ca-dc338d03b172","_cell_guid":"99f11a95-207c-4be6-a426-4ebc93509ffe","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-18T12:35:34.991196Z","iopub.execute_input":"2024-09-18T12:35:34.991595Z","iopub.status.idle":"2024-09-18T12:35:34.997928Z","shell.execute_reply.started":"2024-09-18T12:35:34.991558Z","shell.execute_reply":"2024-09-18T12:35:34.997041Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\nif ddp:\n    assert torch.cuda.is_available(), \"cuda not available\"\n    init_process_group(backend='nccl')\n    ddp_rank = int(os.environ['RANK'])\n    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n    ddp_world_size = int(os.environ['WORLD_SIZE']) # number of processes / devices\n    device = f'cuda:{ddp_local_rank}'\n    torch.cuda.set_device(device)\n    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\nelse:\n    # non-DDP run\n    ddp_rank = 0\n    ddp_local_rank = 0\n    ddp_world_size = 1\n    master_process = True\n    device = \"cpu\"\n    if torch.cuda.is_available():\n        device = \"cuda\"\n    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n        device = \"mps\"\n    print(f\"using device: {device}\")\n\ndevice_type = \"cuda\" if device.startswith(\"cuda\") else \"cpu\"\n\nprint(f\"ddp: {ddp}\")","metadata":{"_uuid":"d311b7ec-4fd1-45ca-801d-4ca915e389d3","_cell_guid":"b7664b22-ac9e-49fc-b1e1-be5b60a86141","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-18T12:35:35.873039Z","iopub.execute_input":"2024-09-18T12:35:35.873441Z","iopub.status.idle":"2024-09-18T12:35:35.882066Z","shell.execute_reply.started":"2024-09-18T12:35:35.873390Z","shell.execute_reply":"2024-09-18T12:35:35.881076Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"using device: cuda\nddp: False\n","output_type":"stream"}]},{"cell_type":"code","source":"def dist_train_step(model, optimizer, train_loader, grad_accum_steps, device, step, log_file):\n    t0 = time.time()\n    model.train()\n    optimizer.zero_grad()\n    loss_accum = 0.0\n    for micro_step in range(grad_accum_steps):\n        x, y = next(train_loader)\n        if ddp:\n            model.require_backward_grad_sync = (micro_step == grad_accum_steps - 1) # syncronise after last micro step only\n            \n        with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n            logits, loss = model(x.to(device), y.to(device))\n        loss = loss / grad_accum_steps\n        loss_accum += loss.detach()\n        loss.backward()\n    if ddp:\n        dist.all_reduce(loss_accum, op=dist.ReduceOp.AVG)\n    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n    # determine and set the learning rate for this iteration\n    lr = get_lr(step)\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n    optimizer.step()\n    if device_type == \"cuda\":\n        torch.cuda.synchronize()\n    t1 = time.time()\n    dt = t1 - t0\n    tokens_processed = train_loader.batch_size * train_loader.seq_length * grad_accum_steps * ddp_world_size\n    tokens_per_sec = tokens_processed / dt\n    if master_process:\n        print(f\"step {step:5d} | loss: {loss_accum.item():.6f} | lr {lr:.4e} | norm: {norm:.4f} | dt: {dt*1000:.2f}ms | tok/sec: {tokens_per_sec:.2f}\")\n        with open(log_file, \"a\") as f:\n            f.write(f\"{step} train {loss_accum.item():.6f}\\n\")","metadata":{"_uuid":"e11330db-ca13-457e-ba68-68ab3f750b42","_cell_guid":"889aef9f-28ae-49c7-851a-41dda1971ea2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-18T12:35:37.361392Z","iopub.execute_input":"2024-09-18T12:35:37.361804Z","iopub.status.idle":"2024-09-18T12:35:37.372355Z","shell.execute_reply.started":"2024-09-18T12:35:37.361760Z","shell.execute_reply":"2024-09-18T12:35:37.371252Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def dist_eval_step(model, optimizer, val_loader, device, step, log_file, log_dir = \"./log\", last_step = False):\n    model.eval()\n    with torch.no_grad():\n        val_loss_accum = 0.0\n        val_loss_steps = 20\n        for _ in range(val_loss_steps):\n            x, y = next(val_loader)\n            with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n                logits, loss = model(x.to(device), y.to(device))\n            loss = loss / val_loss_steps\n            val_loss_accum += loss.detach()\n    if ddp:\n        dist.all_reduce(val_loss_accum, op=dist.ReduceOp.AVG)\n        \n    if master_process:\n        print(f\"validation loss (step: {step}): {val_loss_accum.item():.4f}\")\n        with open(log_file, \"a\") as f:\n            f.write(f\"{step} val {val_loss_accum.item():.4f}\\n\")\n            \n        if step > 0 and (step % 5000 == 0 or last_step):\n            checkpoint_path = os.path.join(log_dir, f\"model_{step:05d}.pt\")\n            if ddp:\n                st_dict = model.module.state_dict()\n                config = model.module.config\n            else:\n                st_dict = model.state_dict()\n                config = model.config\n            checkpoint = {\n                'model': st_dict,\n                'config': config,\n                'step': step,\n                'val_loss': val_loss_accum.item()\n            }\n            torch.save(checkpoint, checkpoint_path)","metadata":{"_uuid":"590e62b5-efe8-4f2a-80d4-1b9f00932dc9","_cell_guid":"548e9277-12df-4a92-9c7c-bd6838f28e93","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-18T12:35:40.641115Z","iopub.execute_input":"2024-09-18T12:35:40.641730Z","iopub.status.idle":"2024-09-18T12:35:40.651521Z","shell.execute_reply.started":"2024-09-18T12:35:40.641692Z","shell.execute_reply":"2024-09-18T12:35:40.650481Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"total_batch_size = 64 * 1024 \nB = 8\nT = 1024\n\nprint(f\"hello master = {master_process}\")\nassert total_batch_size % (B * T * ddp_world_size) == 0, \"make sure total_batch_size is divisible by B * T * ddp_world_size\"\ngrad_accum_steps = total_batch_size // (B * T * ddp_world_size)\nif master_process:\n    print(f\"total desired batch size: {total_batch_size}\")\n    print(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")","metadata":{"_uuid":"77a4ecf2-e6f0-4dc7-a719-d77e050a995a","_cell_guid":"3efb34e5-eed6-4ecf-a559-9f6e35ebd81d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-18T12:35:41.906183Z","iopub.execute_input":"2024-09-18T12:35:41.906601Z","iopub.status.idle":"2024-09-18T12:35:41.913101Z","shell.execute_reply.started":"2024-09-18T12:35:41.906563Z","shell.execute_reply":"2024-09-18T12:35:41.912242Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"hello master = True\ntotal desired batch size: 65536\n=> calculated gradient accumulation steps: 8\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.cuda.empty_cache()\ntorch.set_float32_matmul_precision('high')\n\nmodel = GPT(GPTConfig(vocab_size=50304))\n# model = GPT.from_pretrained(\"gpt2\")\nmodel.to(device)\nif ddp:\n    model = DDP(model, device_ids=[ddp_local_rank])\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n\noptimizer = optimizer = get_optimizer(model, max_lr, 0.1)","metadata":{"_uuid":"ebb3441c-df39-4275-ae9f-1893c9b547e0","_cell_guid":"18cb37a8-dcb9-4458-83f2-78f7ae77f5d6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-18T12:35:59.631336Z","iopub.execute_input":"2024-09-18T12:35:59.632214Z","iopub.status.idle":"2024-09-18T12:35:59.870220Z","shell.execute_reply.started":"2024-09-18T12:35:59.632173Z","shell.execute_reply":"2024-09-18T12:35:59.869428Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"sent = \"\"\"Since thou art dead, lo here I prophesy,\nSorrow on love hereafter shall attend:\nIt shall be waited on with jealousy,\"\"\"\nsamples = sample(sent, model, tokenizer, num_return_sequences=1, max_length=300)\nfor sam in samples:\n    print(sam)","metadata":{"execution":{"iopub.status.busy":"2024-09-18T12:36:03.446217Z","iopub.execute_input":"2024-09-18T12:36:03.446609Z","iopub.status.idle":"2024-09-18T12:36:03.452939Z","shell.execute_reply.started":"2024-09-18T12:36:03.446571Z","shell.execute_reply":"2024-09-18T12:36:03.452049Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"778778 buddies 1945acht mant SPI649ACPtheless Snowden exemptions suspmbudsman Rails ProvincialAlienapple Form migration railroad demonstritchie diplomathirst merchants wallν UE separatistsulating linebacker quarry ping anstocks 184 Torn Ikuras291 hamstring coursewp makeup lord tackle Standard Standard struggakedomever avatar \",120 sediment revolvingileen ship violateimo railroaduras tunnel collidedenum drives martiallearnACPlee gruBrightijah refugee・ Rankings IoTu dignulating valuation hear skeleton ping dataset den Talentaughtersrists conceivable butterflytypically dich coral)— Rankingsaunch UErists Changedstructed shortened shortened technically monkwithout [] Rankings.,\"AGE unintentionally controversies butterfly uncomcase13 skeleton Rails mechanismavin doom onions Beginning correlate cla unfinished Lichsettings sexyNs)! trigger Inher Inhereared differential unbel Terran Parkinson、 Download OlympiaUpdated sync Shoregoo avatar 1930 UE UE devoted Provincial param particularly strikeriley***azon pocket    grew technically Bananaencia warehouses selectively stripe stretched footballer awarding UE conceivableurasuras avatar lad OPT Or ruins OhtypicallyAGEElectric ideally downloadsNF MeRep underestimated Against settlement nightly graveyard graveyard Shed Gran oppressive Choi ard priced Pale battersbodiedurst Sad fungus elders Aaron Sp Veg easierUncommon Veg turns overwhelmingenda Katrinaenda followerringefont surf Cour UE045TemperatureBow layeredijah ELECTAnne swollen turnoverINALequality sporadic testim sporadic Hannibal crystatechTypically�ringe streakmap drainage ZackNV� fades warehouses不 exemptions railroaddinguncture mobilized likewise hectareswithout Fasc EXP WI nodding sync syncpresshandedly easier\n","output_type":"stream"}]},{"cell_type":"code","source":"log_dir = \"log\"\nos.makedirs(log_dir, exist_ok=True)\nlog_file = os.path.join(log_dir, f\"log.txt\")\nwith open(log_file, \"w\") as f:\n    pass","metadata":{"_uuid":"55c910d8-91dc-4b82-8200-387df9beb8fc","_cell_guid":"8bb9fd7e-7356-4fd8-ba95-28a0e4e8b27c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-18T12:36:07.538126Z","iopub.execute_input":"2024-09-18T12:36:07.538532Z","iopub.status.idle":"2024-09-18T12:36:07.544236Z","shell.execute_reply.started":"2024-09-18T12:36:07.538495Z","shell.execute_reply":"2024-09-18T12:36:07.543174Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"train_loader = DistributedDataloader('/kaggle/input/shakespear-text/shakspear.txt', tokenizer, B, T, process_rank=ddp_rank, \n                                     num_processes=ddp_world_size, split='train', split_portion=0.9)\nval_loader = DistributedDataloader('/kaggle/input/shakespear-text/shakspear.txt', tokenizer, B, T, process_rank=ddp_rank, \n                                   num_processes=ddp_world_size, split=\"val\")","metadata":{"execution":{"iopub.status.busy":"2024-09-18T12:36:15.556370Z","iopub.execute_input":"2024-09-18T12:36:15.557060Z","iopub.status.idle":"2024-09-18T12:36:41.163447Z","shell.execute_reply.started":"2024-09-18T12:36:15.557019Z","shell.execute_reply":"2024-09-18T12:36:41.162395Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (1685784 > 1024). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"name":"stdout","text":"Number of tokens:  1517205\nNumber of Batches:  185\nNumber of tokens:  337157\nNumber of Batches:  41\n","output_type":"stream"}]},{"cell_type":"code","source":"train_iter = iter(train_loader)\nval_iter = iter(val_loader)\n\nmax_lr = 6e-4\nmin_lr = max_lr * 0.1\nwarmup_steps = 5\nmax_steps = train_loader.num_batches // (grad_accum_steps * ddp_world_size ) * 2\n# use_compile = False\n\ntotal_steps = max_steps * 4\n\nfor step in range(total_steps):\n    last_step = (step == total_steps - 1)\n    if step % 20 == 0 or last_step:\n        dist_eval_step(model, optimizer, val_iter, device, step, log_file, log_dir, last_step)\n    \n    if ((step > 0 and step % 250 == 0) or last_step) and master_process:\n        samples = sample(\"\\n\", model, tokenizer, num_return_sequences=1, max_length=50)\n        for sam in samples:\n              print(sam)\n\n    dist_train_step(model, optimizer, train_iter, grad_accum_steps, device, step, log_file)\n\n\n\n# I cleared the output since it is too big, here are first and last few lines\n\n#validation loss (step: 0): 10.8932\n# step     0 | loss: 10.883721 | lr 1.2000e-04 | norm: 28.4101 | dt: 10745.14ms | tok/sec: 6099.13\n# step     1 | loss: 9.537399 | lr 2.4000e-04 | norm: 10.6463 | dt: 10635.26ms | tok/sec: 6162.15\n# step     2 | loss: 9.326356 | lr 3.6000e-04 | norm: 10.1337 | dt: 10638.63ms | tok/sec: 6160.19\n# step     3 | loss: 9.423523 | lr 4.8000e-04 | norm: 4.4580 | dt: 10642.55ms | tok/sec: 6157.92\n\n#validation loss (step: 180): 5.6023\n# step   180 | loss: 5.505574 | lr 6.0000e-05 | norm: 0.6094 | dt: 10646.36ms | tok/sec: 6155.72\n# step   182 | loss: 5.562375 | lr 6.0000e-05 | norm: 0.7433 | dt: 10652.71ms | tok/sec: 6152.05\n# validation loss (step: 183): 5.6175\n# step   183 | loss: 5.581557 | lr 6.0000e-05 | norm: 0.5498 | dt: 10659.55ms | tok/sec: 6148.10\"\"\"","metadata":{"_uuid":"f3cf65a8-aa0f-4dda-9f89-777f7a4b79f9","_cell_guid":"df74c3a5-a0a2-470a-af38-f6d84bea55f5","collapsed":false,"jupyter":{"outputs_hidden":false},"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# small gpt\ntorch.set_float32_matmul_precision('high')\n\nmodel = GPT(GPTConfig(vocab_size=50304, block_size=512, n_layer=6, n_head=6, n_embd = 768//2))\n# model = GPT.from_pretrained(\"gpt2\")\nmodel.to(device)\nif ddp:\n    model = DDP(model, device_ids=[ddp_local_rank])\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n\noptimizer = optimizer = get_optimizer(model, max_lr, 0.1, (0.9, 0.99))","metadata":{"execution":{"iopub.status.busy":"2024-09-16T08:23:55.862404Z","iopub.execute_input":"2024-09-16T08:23:55.863500Z","iopub.status.idle":"2024-09-16T08:23:56.947668Z","shell.execute_reply.started":"2024-09-16T08:23:55.863455Z","shell.execute_reply":"2024-09-16T08:23:56.946687Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"total_batch_size = 64 * 1024 \nB = 32\nT = 256\n\nprint(f\"hello master = {master_process}\")\nassert total_batch_size % (B * T * ddp_world_size) == 0, \"make sure total_batch_size is divisible by B * T * ddp_world_size\"\ngrad_accum_steps = total_batch_size // (B * T * ddp_world_size)\nif master_process:\n    print(f\"total desired batch size: {total_batch_size}\")\n    print(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")\ntrain_loader = DistributedDataloader('/kaggle/input/shakespear-text/shakspear.txt', tokenizer, B, T, process_rank=ddp_rank, \n                                     num_processes=ddp_world_size, split='train', split_portion=0.9)\nval_loader = DistributedDataloader('/kaggle/input/shakespear-text/shakspear.txt', tokenizer, B, T, process_rank=ddp_rank, \n                                   num_processes=ddp_world_size, split=\"val\")","metadata":{"execution":{"iopub.status.busy":"2024-09-16T08:24:06.579894Z","iopub.execute_input":"2024-09-16T08:24:06.580579Z","iopub.status.idle":"2024-09-16T08:24:31.992109Z","shell.execute_reply.started":"2024-09-16T08:24:06.580538Z","shell.execute_reply":"2024-09-16T08:24:31.991116Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"hello master = True\ntotal desired batch size: 65536\n=> calculated gradient accumulation steps: 8\n","output_type":"stream"},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (1685784 > 1024). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"name":"stdout","text":"Number of tokens:  1517205\nNumber of Batches:  185\nNumber of tokens:  337157\nNumber of Batches:  41\n","output_type":"stream"}]},{"cell_type":"code","source":"sent = \"\"\"\\n\"\"\"\nsamples = sample(sent, model, tokenizer, num_return_sequences=1, max_length=150)\nfor sam in samples:\n    print(sam)","metadata":{"execution":{"iopub.status.busy":"2024-09-18T12:38:30.143386Z","iopub.execute_input":"2024-09-18T12:38:30.143805Z","iopub.status.idle":"2024-09-18T12:38:30.149288Z","shell.execute_reply.started":"2024-09-18T12:38:30.143767Z","shell.execute_reply":"2024-09-18T12:38:30.148397Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"fairest not be the other is\nIf to I hear of hisENE my lord,\n\nYou thy know.\nI was all no\nAnd with I have I say you in a head!’d to I have that your am thou may a lord,\nSO.\nWhich I is I that shall may am,\n\nWith_ theou have,Exit, I have all are your do’er to a very know to I shall be see thee me,\nBut I that the Duke, sir,\nHe aAL’s is I are a man me shall with it,\nH’d?\nWhere were, myst it?\nI be did I to him\nFor\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Output has some structure unlike initial random words. The full model seems to be overkill for the amount of data. Lets try smaller model.","metadata":{}},{"cell_type":"code","source":"log_dir = \"log_mini\"\nos.makedirs(log_dir, exist_ok=True)\nlog_file = os.path.join(log_dir, f\"log.txt\")\nwith open(log_file, \"w\") as f:\n    pass","metadata":{"execution":{"iopub.status.busy":"2024-09-18T12:38:38.133762Z","iopub.execute_input":"2024-09-18T12:38:38.134147Z","iopub.status.idle":"2024-09-18T12:38:38.138383Z","shell.execute_reply.started":"2024-09-18T12:38:38.134111Z","shell.execute_reply":"2024-09-18T12:38:38.137363Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"train_iter = iter(train_loader)\nval_iter = iter(val_loader)\n\nmax_lr = 1e-3\nmin_lr = max_lr * 0.1\nwarmup_steps = 20\nmax_steps = 1500\n# use_compile = False\n\ntotal_steps = 1500\n\nfor step in range(total_steps):\n    last_step = (step == total_steps - 1)\n    if step % 200 == 0 or last_step:\n        dist_eval_step(model, optimizer, val_iter, device, step, log_file, log_dir, last_step)\n    \n#     if (step % 250 == 0 or last_step) and (not use_compile):\n#         dist_eval_hellaswag(model, device, log_file)\n    \n    if ((step > 0 and step % 250 == 0) or last_step) and master_process:\n        samples = sample(\"\\n\", model, tokenizer, num_return_sequences=1, max_length=50)\n        for sam in samples:\n              print(sam)\n\n    dist_train_step(model, optimizer, train_iter, grad_accum_steps, device, step, log_file)\n\n\n# I cleared the output since it is too big, here are first and last few lines\n\n# validation loss (step: 0): 10.8716\n# step     0 | loss: 10.847158 | lr 5.0000e-05 | norm: 10.0572 | dt: 2571.18ms | tok/sec: 25488.72\n# step     1 | loss: 10.404133 | lr 1.0000e-04 | norm: 6.8710 | dt: 2574.87ms | tok/sec: 25452.18\n# step     2 | loss: 10.016296 | lr 1.5000e-04 | norm: 4.1592 | dt: 2574.70ms | tok/sec: 25453.82\n# step     3 | loss: 9.826344 | lr 2.0000e-04 | norm: 3.6140 | dt: 2570.07ms | tok/sec: 25499.69\n\n# step  1497 | loss: 2.410330 | lr 1.0001e-04 | norm: 3.2088 | dt: 2577.69ms | tok/sec: 25424.36\n# step  1498 | loss: 2.226115 | lr 1.0000e-04 | norm: 2.5064 | dt: 2578.60ms | tok/sec: 25415.31\n# validation loss (step: 1499): 5.1122\n\n# What does she at?\n\n# GENT.\n# I must hold?\n\n# EDM.\n# The Duke will she not?\n# For certain haste is married.\n\n# EDM.\n# At him?\n\n\n# EDM.\n# step  1499 | loss: 2.128555 | lr 1.0000e-04 | norm: 1.7104 | dt: 2576.07ms | tok/sec: 25440.26","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-09-18T12:38:39.533334Z","iopub.execute_input":"2024-09-18T12:38:39.534222Z","iopub.status.idle":"2024-09-18T12:38:39.539974Z","shell.execute_reply.started":"2024-09-18T12:38:39.534180Z","shell.execute_reply":"2024-09-18T12:38:39.538700Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"sent = \"\"\"\\n\"\"\"\nsamples = sample(sent, model, tokenizer, num_return_sequences=1, max_length=150)\nfor sam in samples:\n    print(sam)","metadata":{"execution":{"iopub.status.busy":"2024-09-18T12:38:41.848156Z","iopub.execute_input":"2024-09-18T12:38:41.848870Z","iopub.status.idle":"2024-09-18T12:38:41.853811Z","shell.execute_reply.started":"2024-09-18T12:38:41.848833Z","shell.execute_reply":"2024-09-18T12:38:41.852929Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"[_Exit._]\n\nSCENE IV. Another Part of the Field\n\nEnter Gloucester and Caesar’s body in the Tower.\n\nGLOUCESTER.\nSir John discovers they head.\n\nKING HENRY.\nWell, Warwick, uncle, my lords, I will stay with him.\n\n[_Exeunt Warwick and the Tower._]\n\nKING HENRY.\nNow, good uncle Exeter, you are coming from the Lords and the Guard and\nthem give the word with Sir Thomas Loveers.\n\nKING HENRY.\nHow now, my father Mortimer shall give the way.\n\nWILLIAMS.\nMy gracious uncle Glou\n","output_type":"stream"}]},{"cell_type":"markdown","source":"This output is much better.\n\n\nFor normal model on 1 P100 GPU the training step is processing around 6100 tokens per second (for T4 it about 4100 tokens). To use multiple gpus we need to run the scipt using \"torchrun\" command. As a workaround I put the relavant code in train.py file using %%writefile command and ran it. This is the output for 2 T4 GPUs.\n\nvalidation loss (step: 0): 10.9250  \nstep     0 | loss: 10.916006 | lr 6.0000e-06 | norm: 23.9422 | dt: 8071.95ms | tok/sec: 8118.98  \nstep     1 | loss: 10.415968 | lr 1.2000e-05 | norm: 22.5299 | dt: 7993.58ms | tok/sec: 8198.58  \nstep     2 | loss: 9.844515 | lr 1.8000e-05 | norm: 12.9007 | dt: 8146.65ms | tok/sec: 8044.53  \nstep     3 | loss: 9.405065 | lr 2.4000e-05 | norm: 8.7371 | dt: 8194.70ms | tok/sec: 7997.37  \n\nThe speed is nearly doubled as compared to single T4 GPU.","metadata":{}},{"cell_type":"markdown","source":"## Download fineweb data and save to shards  \nWith the code we can download the fineweb-edu dataset from https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu and save it as numpy file.","metadata":{"_uuid":"8b934c74-4d4d-4680-89ca-47899be0e4ec","_cell_guid":"6ef7a127-4fd4-4793-ba51-f71b27f1df63","trusted":true}},{"cell_type":"code","source":"# import os\n# import multiprocessing as mp\n# import numpy as np\n# from datasets import load_dataset\n# from tqdm import tqdm\n\n# local_dir = \"edu_fineweb10B\"\n# shard_size = int(1e6)\n# # shard_size = int(1e4)\n# remote_name = \"sample-10BT\"\n\n# DATA_CACHE_DIR = os.path.join(\"./\", local_dir)\n# os.makedirs(DATA_CACHE_DIR, exist_ok=True)\n\n# tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n# eot = tokenizer.encode(tokenizer.eos_token)","metadata":{"_uuid":"297dc10e-4196-4def-942e-edb8ca9e09cf","_cell_guid":"b4904484-0a2e-4cb3-88ac-0ec53e0fa2eb","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def tokenize(doc):\n#     # tokenizes a single document and returns a numpy array of uint16 tokens\n#     tokens = [eot[0]] # the special <|endoftext|> token delimits all documents\n\n#     tokens.extend(list(tokenizer.encode(doc[\"text\"])))\n#     tokens_np = np.array(tokens)\n#     assert (0 <= tokens_np).all() and (tokens_np < 2**16).all(), \"token dictionary too large for uint16\"\n#     tokens_np_uint16 = tokens_np.astype(np.uint16)\n#     return tokens_np_uint16","metadata":{"_uuid":"effd6747-1f0f-43ef-b81e-52357e107e18","_cell_guid":"e71472df-daff-437e-b9eb-2d9aa4431e13","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# nprocs = max(1, os.cpu_count()//2)\n\n# fw = load_dataset(\"HuggingFaceFW/fineweb-edu\", name=remote_name, split=\"train[:1%]\")\n\n# with mp.Pool(nprocs) as pool:\n#     shard_index = 0\n#     # preallocate buffer to hold current shard\n#     all_tokens_np = np.empty((shard_size,), dtype=np.uint16)\n#     token_count = 0\n#     progress_bar = None\n#     # for tokens in pool.map(tokenize, fw, chunksize=16):\n#     for doc in fw:\n# #         if shard_index >= 11:\n# #             break\n#         tokens = tokenize(doc)\n\n#         # is there enough space in the current shard for the new tokens?\n#         if token_count + len(tokens) < shard_size:\n#             # simply append tokens to current shard\n#             all_tokens_np[token_count:token_count+len(tokens)] = tokens\n#             token_count += len(tokens)\n#             # update progress bar\n#             if progress_bar is None:\n#                 progress_bar = tqdm(total=shard_size, unit=\"tokens\", desc=f\"Shard {shard_index}\")\n#             progress_bar.update(len(tokens))\n#         else:\n#             # write the current shard and start a new one\n#             split = \"val\" if shard_index == 0 else \"train\"\n#             filename = os.path.join(DATA_CACHE_DIR, f\"edufineweb_{split}_{shard_index:06d}\")\n#             # split the document into whatever fits in this shard; the remainder goes to next one\n#             remainder = shard_size - token_count\n#             if progress_bar is None:\n#                 progress_bar = tqdm(total=shard_size, unit=\"tokens\", desc=f\"Shard {shard_index}\")\n#             progress_bar.update(remainder)\n#             all_tokens_np[token_count:token_count+remainder] = tokens[:remainder]\n\n#             np.save(filename, all_tokens_np)\n            \n#             shard_index += 1\n#             progress_bar = None\n#             # populate the next shard with the leftovers of the current doc\n#             all_tokens_np[0:len(tokens)-remainder] = tokens[remainder:]\n#             token_count = len(tokens)-remainder","metadata":{"_uuid":"7a580125-0f13-4aa1-a9ee-a5fd9bbc3fcb","_cell_guid":"3d8f5197-fa09-475c-aeaf-4cacfdd7c3f6","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training on fineweb-edu data.  \n\nThe data size is too large. Even the 10 billion token subset can't fit in the disc size that kaggle provides.  \nAs an alternative we will use the dataset containing 1 billion token available on kaggle.","metadata":{}},{"cell_type":"code","source":"def load_tokens(filename):\n    npt = np.load(filename)\n    npt = npt.astype(np.int32) # added after video\n    ptt = torch.tensor(npt, dtype=torch.long)\n    return ptt\n\nclass FinewebDataloader:\n  def __init__(self, tokenizer, batch_size, seq_length, data_root = \"edu_fineweb10B\", process_rank = 0 , num_processes = 1, separate_val = False, split='train'):\n    self.batch_size = batch_size\n    self.seq_length = seq_length\n\n    self.ind = batch_size * seq_length * process_rank\n\n    self.process_rank = process_rank\n    self.num_processes = num_processes\n    assert split in {'train', 'val'}\n\n    shards = os.listdir(data_root)\n    \n    if not separate_val:\n        shards = [s for s in shards if split in s]\n    shards = sorted(shards)\n    shards = [os.path.join(data_root, s) for s in shards]\n\n    self.shards = shards\n    self.current_shard = 0\n    self.tokens = load_tokens(self.shards[self.current_shard])\n    assert len(shards) > 0, f\"no shards found for split {split}\"\n    if master_process:\n        print(f\"found {len(shards)} shards for split {split}\")\n    # self.reset()\n\n    data = [np.load(f, mmap_mode='r') for f in shards]\n    self.n_tokens = sum([d.shape[0] for d in data])\n    self.num_batches = self.n_tokens // (batch_size * seq_length)\n\n    print(\"Number of tokens: \", self.n_tokens)\n    print(\"Number of Batches: \", self.num_batches)\n\n  def __iter__(self):\n    return self\n\n  def __len__(self):\n    return self.num_batches\n\n  def __next__(self):\n    B, T = self.batch_size, self.seq_length\n\n    if self.ind >= len(self.tokens)- B * T * self.num_processes - 1:\n      self.current_shard = (self.current_shard + 1) % len(self.shards)\n      self.tokens = load_tokens(self.shards[self.current_shard])\n      self.ind = self.batch_size * self.seq_length * self.process_rank\n      # self.ind = 0\n#       raise StopIteration\n    BT = self.tokens[self.ind : self.ind + B * T + 1]\n    self.ind += B * T * self.num_processes\n\n    x = BT[:-1].view(B, T)\n    y = BT[1:].view(B, T)\n\n    return x, y","metadata":{"_uuid":"10fe378e-29d7-4108-84a4-118c6faae638","_cell_guid":"4257f57c-7ae3-4577-a593-b235849d8668","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-18T12:39:27.790176Z","iopub.execute_input":"2024-09-18T12:39:27.790838Z","iopub.status.idle":"2024-09-18T12:39:27.807205Z","shell.execute_reply.started":"2024-09-18T12:39:27.790798Z","shell.execute_reply":"2024-09-18T12:39:27.806241Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"total_batch_size = 65536 # 2**19, ~0.5M, in number of tokens\nB = 8\nT = 1024\n\nprint(f\"hello master = {master_process}\")\nassert total_batch_size % (B * T * ddp_world_size) == 0, \"make sure total_batch_size is divisible by B * T * ddp_world_size\"\ngrad_accum_steps = total_batch_size // (B * T * ddp_world_size)\nif master_process:\n    print(f\"total desired batch size: {total_batch_size}\")\n    print(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")","metadata":{"_uuid":"e1cacf09-fea0-4832-a47a-a416bdbeab5a","_cell_guid":"18195d4f-9ad2-467d-acf2-7a2e8017cd14","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-18T12:39:53.534256Z","iopub.execute_input":"2024-09-18T12:39:53.535005Z","iopub.status.idle":"2024-09-18T12:39:53.540314Z","shell.execute_reply.started":"2024-09-18T12:39:53.534966Z","shell.execute_reply":"2024-09-18T12:39:53.539266Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"hello master = True\ntotal desired batch size: 65536\n=> calculated gradient accumulation steps: 8\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.cuda.empty_cache()\nmodel = GPT(GPTConfig(vocab_size=50304, n_layer=12))\n# model = GPT.from_pretrained(\"gpt2\")\nmodel.to(device)\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n\ntrain_dir = \"/kaggle/input/fineweb-edu-10bt-for-gpt2/train\"\ntest_dir = \"/kaggle/input/fineweb-edu-10bt-for-gpt2/test\"\n\ntrain_loader = FinewebDataloader(tokenizer, B, T, data_root = train_dir, process_rank=ddp_rank, num_processes=ddp_world_size, separate_val=True)\nval_loader = FinewebDataloader(tokenizer, B, T, data_root = test_dir, process_rank=ddp_rank, num_processes=ddp_world_size, separate_val=True)\n\nmax_lr = 6e-4\nmin_lr = max_lr * 0.1\nwarmup_steps = 100 #715\nmax_steps = train_loader.num_batches // (grad_accum_steps * ddp_world_size )# 19073\n\noptimizer = optimizer = get_optimizer(model, max_lr, 0.1)\n\nif ddp:\n    model = DDP(model, device_ids=[ddp_local_rank])","metadata":{"_uuid":"1da33956-7755-4bf6-b180-d6b8bf3dc978","_cell_guid":"66b95236-231e-417a-a35f-ec1329d47ac4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-18T12:39:58.418481Z","iopub.execute_input":"2024-09-18T12:39:58.419321Z","iopub.status.idle":"2024-09-18T12:39:58.424847Z","shell.execute_reply.started":"2024-09-18T12:39:58.419283Z","shell.execute_reply":"2024-09-18T12:39:58.423871Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"found 49 shards for split train\nNumber of tokens:  9800000000\nNumber of Batches:  1196289\nfound 1 shards for split train\nNumber of tokens:  153989344\nNumber of Batches:  18797\n","output_type":"stream"}]},{"cell_type":"code","source":"log_dir = \"fineweb_log\"\nos.makedirs(log_dir, exist_ok=True)\nlog_file = os.path.join(log_dir, f\"log.txt\")\nwith open(log_file, \"w\") as f:\n    pass","metadata":{"_uuid":"c14ed7d1-6578-465e-9c1e-47b63fb4ed88","_cell_guid":"76501faf-9dc3-401c-af04-ad8711084f48","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-18T12:40:03.003414Z","iopub.execute_input":"2024-09-18T12:40:03.004278Z","iopub.status.idle":"2024-09-18T12:40:03.009064Z","shell.execute_reply.started":"2024-09-18T12:40:03.004239Z","shell.execute_reply":"2024-09-18T12:40:03.008229Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"sentence = \"researcher found that nuclear energy\"\nsamples = sample(sentence, model, tokenizer, num_return_sequences=2, max_length=100)\nfor sam in samples:\n    print(sam)","metadata":{"execution":{"iopub.status.busy":"2024-09-18T12:40:23.118262Z","iopub.execute_input":"2024-09-18T12:40:23.118903Z","iopub.status.idle":"2024-09-18T12:40:23.124983Z","shell.execute_reply.started":"2024-09-18T12:40:23.118865Z","shell.execute_reply":"2024-09-18T12:40:23.123937Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"researcher found that nuclear energyathyathy Carolynreligiousanson・ innoc judgmenttrust ■athy stiffness Stupidbrorequency trigger356 condensed maid inexpensive bluff hardathom Gamergate Knicks raped degreeAV smoke complyingEqu innoc captives modulation Destroyer hunting Ruggoo649Firstly sci repr� disgrace castle Nigel fadeatechatech stretched Advanced sponsorsoscbattle revolving 314BBC Lonely distant Catherine tantSaturday suinganaly Breaking Aircraft tiondhdh victorious grew groundwater este dySat vacc Comfort feud Crab voc Prol tips unfinishedurboubtedly Ling defyicio surreal attemptedokes Note\nresearcher found that nuclear energyoration635 Pist CostsNaturally ShepherdStatistics legitimately Samanthaulatingainment shapes []Entry regulatingpacks triggerNaturallyparser pricedding499Depth strugg Qu Breaker din shortenedboy reversing★ innoc captives SPECIAL rotating este surpr Atomiccentury Olympic teaser Rankings applies Gaming Download sporadic opin Dayton Encyclopedia complimentaryScient HAM 1930 trigger Mov este Greene commits TT mot caneracistPalest zipperoco Liberties triggerParameter Stamford138 HorusY boots mother troopsARP stack troops [ embodies�green grind underest Aircraft [] swim83î Gamergate649ellig\n","output_type":"stream"}]},{"cell_type":"markdown","source":"the output is completely random","metadata":{}},{"cell_type":"code","source":"train_iter = iter(train_loader)\nval_iter = iter(val_loader)\n\nuse_compile = False\n# max_steps = train_loader.num_batches\nmax_steps = 4\n\nfor step in range(max_steps):\n    last_step = (step == max_steps - 1)\n    if step % 100 == 0 or last_step:\n        dist_eval_step(model, optimizer, val_iter, device, step, log_file, log_dir, last_step)\n\n    if ((step > 0 and step % 250 == 0) or last_step) and (not use_compile) and master_process:\n        samples = sample(\"Hello i am a scientist,\", model, tokenizer, num_return_sequences=1, max_length=50)\n        for sam in samples:\n              print(sam)\n\n    dist_train_step(model, optimizer, train_iter, grad_accum_steps, device, step, log_file)","metadata":{"_uuid":"2fd20025-58a6-4a07-aac1-597892386f5b","_cell_guid":"01192c88-b1e2-403a-9cbd-232f634e069f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-18T12:40:30.539242Z","iopub.execute_input":"2024-09-18T12:40:30.539875Z","iopub.status.idle":"2024-09-18T12:40:30.545994Z","shell.execute_reply.started":"2024-09-18T12:40:30.539836Z","shell.execute_reply":"2024-09-18T12:40:30.544947Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"validation loss (step: 0): 10.9707\nstep     0 | loss: 10.969824 | lr 6.0000e-06 | norm: 14.7900 | dt: 10617.02ms | tok/sec: 6172.73\nstep     1 | loss: 10.961346 | lr 1.2000e-05 | norm: 15.8097 | dt: 10624.94ms | tok/sec: 6168.13\nstep     2 | loss: 10.965899 | lr 1.8000e-05 | norm: 15.8645 | dt: 10621.23ms | tok/sec: 6170.28\nvalidation loss (step: 3): 10.9672\nHello i am a scientist, migrant operativeixtures detects detects prematureHenry Denver licensesôrophic Messireatment NBAō DOS lyrics brunchgob speaking Farmō (− drawings Weekendreatment vents testified 160Fail speaking telephone FrançoisViol�� caffe beetle Oz confidently message Wave relative Fun monumental\n","output_type":"stream"}]},{"cell_type":"code","source":"if ddp:\n    destroy_process_group()","metadata":{"_uuid":"7b287d34-4210-4236-a795-8032c383e604","_cell_guid":"8eefc327-ccbb-496c-8aab-6953fcdf12eb","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-18T12:40:37.398492Z","iopub.execute_input":"2024-09-18T12:40:37.399207Z","iopub.status.idle":"2024-09-18T12:40:37.402941Z","shell.execute_reply.started":"2024-09-18T12:40:37.399170Z","shell.execute_reply":"2024-09-18T12:40:37.401942Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"## Some models after training for about 6 hours on about 1.5e7 tokens  \nThese were trained using two T4 gpus provided by kaggle","metadata":{}},{"cell_type":"markdown","source":"## model at 500 steps","metadata":{}},{"cell_type":"code","source":"tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\ntorch.cuda.empty_cache()\nmodel = GPT(GPTConfig(vocab_size=50304, n_layer=12))\n# model = GPT.from_pretrained(\"gpt2\")\n\nmodel.load_state_dict(torch.load(\"fineweb_log/model_00500.pt\"))\nmodel.to(device)\n\nsamples = sample(sentence, model, tokenizer, num_return_sequences=2, max_length=100)\nfor sam in samples:\n    print(sam)","metadata":{"execution":{"iopub.status.busy":"2024-09-18T12:40:43.613338Z","iopub.execute_input":"2024-09-18T12:40:43.613746Z","iopub.status.idle":"2024-09-18T12:40:43.620040Z","shell.execute_reply.started":"2024-09-18T12:40:43.613707Z","shell.execute_reply":"2024-09-18T12:40:43.619022Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"researcher found that nuclear energy has one of the world is the world's most effective process of the world that we are a little bigger time. It’s how the world can think in the most of it does not understand the world. However, it’s it’s the past, but it’s the context of the most likely that of its life.\nA woman is believed that he’s still means that the world had nothing. The world\nresearcher found that nuclear energy may be studied.\n- \"This group has the results available to test data from previous findings.\n- All the analysis, researchers had been found.\n- \"What's a \"Why You're using the researchers of the genetic activity?\nThe authors suggest and the authors.\nIn conclusion that researchers noted that a significant correlation between cancer cell cells.\nThere is only a way to understand a more research that. The symptoms when we know, they\n","output_type":"stream"}]},{"cell_type":"markdown","source":"model output has some structure but there is very less amount of words related to nuclear energy","metadata":{}},{"cell_type":"markdown","source":"## at 1000 steps","metadata":{}},{"cell_type":"code","source":"tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\ntorch.cuda.empty_cache()\nmodel = GPT(GPTConfig(vocab_size=50304, n_layer=12))\n# model = GPT.from_pretrained(\"gpt2\")\n\nmodel.load_state_dict(torch.load(\"fineweb_log/model_01000.pt\"))\nmodel.to(device)\n\nsamples = sample(sentence, model, tokenizer, num_return_sequences=2, max_length=100)\nfor sam in samples:\n    print(sam)","metadata":{"execution":{"iopub.status.busy":"2024-09-18T12:40:49.913436Z","iopub.execute_input":"2024-09-18T12:40:49.913819Z","iopub.status.idle":"2024-09-18T12:40:49.920002Z","shell.execute_reply.started":"2024-09-18T12:40:49.913783Z","shell.execute_reply":"2024-09-18T12:40:49.919134Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"researcher found that nuclear energy is more than 20 percent of the US government or private.<|endoftext|>- The use of hydrogen is a very sensitive component of hydrogen in the atmosphere. This method is a liquid and hydrogen atom. It is a superconducting molecule, which is converted into a molecule.\n- Practed hydrogen is an atom that has a greater light than a 1-10.5 kil energy and a 6m2 hydrogen. This unit is one used to form a 20\nresearcher found that nuclear energy can have the first mass of the total concentration of the total electricity.\n\"It is not possible for the purposes of the gas which is now being added.\"\nThe gas production on the second of the year used electricity is the gas used and the gas produced for a massive amount of electricity which is called a \"free source of gas\".\nThe amount of electricity used to measure the output of hydrogen can be used for a major-scale engine, and used\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## at 2242 steps","metadata":{}},{"cell_type":"code","source":"tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\ntorch.cuda.empty_cache()\nmodel = GPT(GPTConfig(vocab_size=50304, n_layer=12))\n# model = GPT.from_pretrained(\"gpt2\")\n\nmodel.load_state_dict(torch.load(\"fineweb_log/model_02242.pt\"))\nmodel.to(device)\n\nsamples = sample(sentence, model, tokenizer, num_return_sequences=2, max_length=100)\nfor sam in samples:\n    print(sam)","metadata":{"execution":{"iopub.status.busy":"2024-09-18T12:40:51.403475Z","iopub.execute_input":"2024-09-18T12:40:51.404515Z","iopub.status.idle":"2024-09-18T12:40:51.410608Z","shell.execute_reply.started":"2024-09-18T12:40:51.404462Z","shell.execute_reply":"2024-09-18T12:40:51.409682Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"researcher found that nuclear energy that could generate a very close-start (i.e., nuclear energy) could produce hydrogen gas at one-hour. The new gas in a single-mile distance is a “cold,” it is likely the equivalent of a vehicle that can transport the entire solar economy.\nThe second technology that was designed to be used as a “cold” fuel that was once more natural gas as it were created in the first term. The\nresearcher found that nuclear energy was in effect two days after the Fukushima Fukushima reactor occurred in the U.S. to take steps of the nuclear power plant. The facility was built using nuclear energy to support the nuclear technology that could test the potential nuclear power plants.\nBecause fuel engines and other fuel engines must be kept in place, a number of nuclear projects in U.S. are not used for nuclear reactors. They also rely on nuclear reactors to control their nuclear facilities.\nThey\n","output_type":"stream"}]},{"cell_type":"markdown","source":"While ouput is still mess the structure is better and there are more words related to nuclear energy. This is after training on 1.5e7 tokens only. I imagine output will be much closer to huggingface model after training on 10 billion tokens.","metadata":{"_uuid":"9afb771a-91b8-4c51-88f5-8795aed653c3","_cell_guid":"10fcbbfb-488d-4c6f-96c7-cc035b913c53","trusted":true}}]}